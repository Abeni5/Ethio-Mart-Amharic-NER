{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94c010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1941cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\EthioMart-Amharic-NER\\notebooks\n",
      "Files: ['analysis_and_finetune.ipynb', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec79f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = \"../data/labeled_telegram_product_price_location.txt\"\n",
    "\n",
    "def parse_conll(filepath):\n",
    "    tokens, ner_tags = [], []\n",
    "    token_list, tag_list = [], []\n",
    "\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if token_list:\n",
    "                    tokens.append(token_list)\n",
    "                    ner_tags.append(tag_list)\n",
    "                    token_list, tag_list = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token, tag = parts[0], parts[1]\n",
    "                    token_list.append(token)\n",
    "                    tag_list.append(tag)\n",
    "    \n",
    "    return tokens, ner_tags\n",
    "\n",
    "tokens, tags = parse_conll(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87cb7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label2id and id2label mappings\n",
    "unique_labels = sorted(set(tag for seq in tags for tag in seq))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Convert tag strings to integers\n",
    "tags_ids = [[label2id[tag] for tag in seq] for seq in tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4810847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2532\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 634\n",
      "    })\n",
      "})\n",
      "Example labels: ['B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PRICE', 'I-PRICE', 'I-PRICE', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens, tags_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": train_tokens, \"ner_tags\": train_tags}),\n",
    "    \"validation\": Dataset.from_dict({\"tokens\": val_tokens, \"ner_tags\": val_tags}),\n",
    "})\n",
    "\n",
    "# Confirm working\n",
    "print(dataset)\n",
    "print(\"Example labels:\", [id2label[i] for i in dataset['train'][0]['ner_tags']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8068a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(label2id.keys())\n",
    "num_labels = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cecf679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ecfc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SentencePiece is working!\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(\"✅ SentencePiece is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b525e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.31.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e622a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/Davlan/bert-base-multilingual-cased-ner-hrl/5cff81bec8c8efca8549b3d843f9b94a404e2a747c71c7343261dcfe9b2ba86c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1750625939&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDYyNTkzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9EYXZsYW4vYmVydC1iYXNlLW11bHRpbGluZ3VhbC1jYXNlZC1uZXItaHJsLzVjZmY4MWJlYzhjOGVmY2E4NTQ5YjNkODQzZjliOTRhNDA0ZTJhNzQ3YzcxYzczNDMyNjFkY2ZlOWIyYmE4NmM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=fvt5ncjUBb94ShT4hGgNJO%7Ei04Ny8ALU3EgtkCbp4NYcKQWwD%7EirxUiyjoXiFWxF44czFw4wpDHxUvD%7EiFWJ%7ERGhGJCjPf8slcGuMv65ArYS155Zka9H1NqFZfqdrk0NgJf0lEXMUT2hl12ur9-lCGlGxeDvvDyyD7a0cu7ciczx%7E1X-kTLr11Hj-sls5DGADtLgg-di7eq76mtKvUy3X4BcEvFfs4TiLeNQh4iYy0nVULVWFLu-w4QGgES50AegDsJ0UdfoZCB0mLk2UusUFoCBKUjs6ChyY8HgI-0hgQYGvWcrNXVrtgU2jxgi6Lzqv5VoIdOFE70VHQWol07CJA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc1451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m../data/labeled_telegram_product_price_location.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Step 4: Define labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m label_list = \u001b[43mdataset\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].features[\u001b[33m\"\u001b[39m\u001b[33mner_tags\u001b[39m\u001b[33m\"\u001b[39m].feature.names\n\u001b[32m     24\u001b[39m num_labels = \u001b[38;5;28mlen\u001b[39m(label_list)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Step 5: Load pretrained tokenizer and model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# analysis_and_finetune.ipynb\n",
    "\n",
    "# Step 1: Install necessary libraries\n",
    "!pip install transformers datasets seqeval -q\n",
    "\n",
    "# Step 2: Import required modules\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 3: Load your CoNLL formatted dataset\n",
    "# Assuming you have a .txt file: 'labeled_amharic.conll'\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"labeled_amharic_train.conll\", \"validation\": \"labeled_amharic_val.conll\"}\n",
    "file_path = \"../data/labeled_telegram_product_price_location.txt\"\n",
    "\n",
    "\n",
    "# Step 4: Define labels\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Step 5: Load pretrained tokenizer and model\n",
    "model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Step 6: Tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label[word_idx] != -100 else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Step 7: Set up training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Step 8: Define Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 9: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 10: Evaluate\n",
    "predictions, labels, _ = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
    "true_preds = [[label_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "print(classification_report(true_labels, true_preds))\n",
    "\n",
    "# Optional: Save the model\n",
    "model.save_pretrained(\"saved_model_amharic_ner\")\n",
    "tokenizer.save_pretrained(\"saved_model_amharic_ner\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
